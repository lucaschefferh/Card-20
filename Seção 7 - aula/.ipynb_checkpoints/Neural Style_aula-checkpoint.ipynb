{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3011349-46d7-4b9d-862b-8086408826bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importações necessárias \n",
    "from torch_snippets import *\n",
    "from torchvision import transforms as T\n",
    "from torch.nn import functional as F\n",
    "from torchvision.models import vgg19  #modelo pré-treinado import torch\n",
    "import torch.nn as nn\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4dc1e-b822-4f67-a12e-10690672ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformações para preparar as imagens\n",
    "preprocess = T.Compose([\n",
    "    T.ToTensor(),  # Converte para tensor\n",
    "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),  #Normalização padrão ImageNet\n",
    "    T.Lambda(lambda x: x.mul_(255))  #Multiplica por 255\n",
    "])\n",
    "\n",
    "#transformações para reverter o pré-processamento \n",
    "postprocess = T.Compose([\n",
    "    T.Lambda(lambda x: x.mul_(1./255)),  # Divide por 255\n",
    "    T.Normalize(mean=[-0.485/0.229, -0.456/0.224,-0.406/0.225], std=[1/0.229,1/0.224,1/0.255]),  # Desnormaliza\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea2ece-35b6-4657-b25d-7d0ccf5e4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe para calcular a Matriz de Gram \n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self,input):\n",
    "        b,c,h,w = input.size()\n",
    "        feat = input.view(b,c,h*w)  #Achata as dimensões espaciais\n",
    "        G = feat@feat.transpose(1,2)  \n",
    "        G.div_(h*w)  #Normaliza pelo tamanho\n",
    "        return G\n",
    "\n",
    "\n",
    "class GramMSELoss(nn.Module):\n",
    "    def forward(self,input,target):\n",
    "        out = F.mse_loss(GramMatrix()(input),target)  #MSE entre matrizes de Gram\n",
    "        return(out)\n",
    "\n",
    "#Classe para extrair features de camadas específicas \n",
    "class vgg19_modified(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        features = list(vgg19(pretrained=True).features)  #Carrega VGG19 pré-treinada\n",
    "        self.features = nn.ModuleList(features).eval()  \n",
    "    \n",
    "    def forward(self, x, layers=[]):\n",
    "        if not layers:\n",
    "            return x\n",
    "        order = np.argsort(layers)\n",
    "        _results, results = [], []\n",
    "        \n",
    "        #passa pela rede e extrai features das camadas especificadas\n",
    "        for ix, model in enumerate(self.features):\n",
    "            x = model(x)\n",
    "            if ix in layers: _results.append(x)\n",
    "        for o in order: results.append(_results[o])\n",
    "        return results if layers is not [] else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76931ad8-88c3-4431-9fce-8add93fae1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instancia o modelo VGG19 modificado e move para o dispositivo\n",
    "vgg = vgg19_modified().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626810f-8b98-47ba-bd2d-13de07ec0ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-29 16:45:34--  https://easydrawingguides.com/wp-content/uploads/2016/10/how-to-draw-an-elephant-featured-image-1200.png\n",
      "2606:4700::6810:966c, 2606:4700::6810:976c, 104.16.150.108, ...\n",
      "Conectando-se a easydrawingguides.com (easydrawingguides.com)|2606:4700::6810:966c|:443... conectado.\n",
      "A requisição HTTP foi enviada, aguardando resposta... 200 OK\n",
      "Tamanho: 56936 (56K) [image/png]\n",
      "Salvando em: ‘how-to-draw-an-elephant-featured-image-1200.png’\n",
      "\n",
      "how-to-draw-an-elep 100%[===================>]  55,60K  --.-KB/s    em 0s      \n",
      "\n",
      "2025-07-29 16:45:35 (132 MB/s) - ‘how-to-draw-an-elephant-featured-image-1200.png’ salvo [56936/56936]\n",
      "\n",
      "--2025-07-29 16:45:35--  https://www.neh.gov/sites/default/files/2022-09/Fall_2022_web-images_Picasso_32.jpg\n",
      "23.21.228.79ww.neh.gov (www.neh.gov)... \n",
      "conectado.-se a www.neh.gov (www.neh.gov)|23.21.228.79|:443... \n",
      "A requisição HTTP foi enviada, aguardando resposta... 200 OK\n",
      "Tamanho: 5309491 (5,1M) [image/jpeg]\n",
      "Salvando em: ‘Fall_2022_web-images_Picasso_32.jpg’\n",
      "\n",
      "Fall_2022_web-image 100%[===================>]   5,06M  4,83MB/s    em 1,0s    \n",
      "\n",
      "2025-07-29 16:45:37 (4,83 MB/s) - ‘Fall_2022_web-images_Picasso_32.jpg’ salvo [5309491/5309491]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://easydrawingguides.com/wp-content/uploads/2016/10/how-to-draw-an-elephant-featured-image-1200.png\n",
    "!wget https://www.neh.gov/sites/default/files/2022-09/Fall_2022_web-images_Picasso_32.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8100f146-a998-4408-ae82-e0962aca5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega as imagens, redimensiona para 512x512 \n",
    "imgs = [Image.open(path).resize((512,512)).convert('RGB') for path in ['picasso.jpg',\n",
    "                                                                       'elephant.png']]\n",
    "style_image, content_image = [preprocess(img).to(device)[None] for img in imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbccabde-3680-4c95-9849-927be9334c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria uma cópia da imagem de conteúdo \n",
    "opt_img = content_image.data.clone()\n",
    "opt_img.requires_grad = True  # Habilita gradientes para otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f7ea5-1c94-4b2b-8a3b-2328b89a5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define quais camadas da VGG19 usar para extrair features\n",
    "style_layers = [0, 5, 10, 19, 28]  \n",
    "content_layers = [21]  \n",
    "loss_layers = style_layers + content_layers  # Todas as camadas usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df525e1d-a309-4140-8147-a5467fc97aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define as funções de perda\n",
    "loss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
    "loss_fns = [loss_fn.to(device) for loss_fn in loss_fns]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c066ec56-90ca-46f1-bbb3-2bacfd0ee301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define os pesos para balancear as perdas de estilo e conteúdo\n",
    "style_weights = [1000/n**2 for n in [64,128,256,512,512]]  \n",
    "content_weights = [1]  \n",
    "weights = style_weights + content_weights  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e4a3ba-2162-4bbf-8722-aea10dfe01db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extrai as features das imagens de referência que servirão como alvos\n",
    "style_target = [GramMatrix()(A).detach() for A in vgg(style_image, style_layers)] \n",
    "content_targets = [A.detach() for A in vgg(content_image, content_layers)]\n",
    "targets = style_target + content_targets  #combina todos os alvos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e474a2b2-2acc-4c0f-973d-f585f7273069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "#Configuração do otimizador \n",
    "max_iters = 500  \n",
    "optimizer = optim.LBFGS([opt_img]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ecc5e-20e6-42fd-83de-57911367f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop principal de otimização\n",
    "iters = 0\n",
    "while iters < max_iters:\n",
    "    def closure():\n",
    "        global iters\n",
    "        iters += 1\n",
    "        optimizer.zero_grad()\n",
    "        out = vgg(opt_img, loss_layers)\n",
    "        # Calcula perdas ponderadas para cada camada\n",
    "        layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n",
    "        loss = sum(layer_losses)  #soma todas as perdas\n",
    "        loss.backward()  #calcula gradientes\n",
    "        return loss\n",
    "    optimizer.step(closure)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
